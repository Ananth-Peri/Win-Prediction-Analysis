{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc27b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing requisite libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "import os as os\n",
    "from skimpy import skim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14413280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the working directory\n",
    "\n",
    "os.chdir(\"E:\\\\Final Project\\\\Other final year project files\\\\WinPrediction (2)\\\\Win Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cccb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files\n",
    "\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6269ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "win_pred = pd.read_excel(\"Win_Prediction_Data.xlsx\", engine=\"openpyxl\"); win_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns and their respective data types\n",
    "\n",
    "# There are missing values in the column \"client category\"\n",
    "\n",
    "win_pred.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291ba87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skim gives a more comprehensive overview of the dataset, including the percentile, data types, missing values, and their distribution\n",
    "\n",
    "skim(win_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047360d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "\n",
    "win_pred.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of categorical variables\n",
    "\n",
    "# Since \"client category\" has missing values, and it is a categorical variable, we'll impute the missing values using mode\n",
    "\n",
    "win_pred.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e22b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're calculating the percentage of missing values in \"client category\"; the value is 0.78%, which is minute, and values can be easily imputed\n",
    "\n",
    "(win_pred[\"Client Category\"].isnull().sum()/len(win_pred[\"Client Category\"]))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f826e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of deal status code\n",
    "\n",
    "# Around 64% of the deals are shown to be in \"lost\"; the outcome is slightly imbalanced (or) skewed towards \"lost\"\n",
    "\n",
    "win_pred[\"Deal Status Code\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a39d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for above code\n",
    "\n",
    "sns.countplot(x=\"Deal Status Code\", data=win_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9b547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap to display missing values in the dataset\n",
    "\n",
    "sns.heatmap(win_pred.isnull(), yticklabels = False,cbar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3920e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at null values in the dataset\n",
    "\n",
    "win_pred.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f049f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputatation of Client Category with mode \"Others\"\n",
    "\n",
    "win_pred[\"Client Category\"].fillna(\"Others\", inplace=True); win_pred.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbef3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleaned data to another excel file for EDA\n",
    "\n",
    "win_pred.to_excel(\"cleaned_win_pred.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c26e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Tabulation to show how many deals are won (or) lost in each client category\n",
    "\n",
    "pd.crosstab(win_pred[\"Client Category\"],win_pred[\"Deal Status Code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a976595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns \"solution type\", \"solution date\", \"sector\"\n",
    "\n",
    "win_pred.drop(win_pred.columns[[2,3,4]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables\n",
    "\n",
    "# Primary reason for dummy variables is to convert the categorical variables and represent them in a quantitative way and fit in models\n",
    "\n",
    "win_pred_dummy = pd.get_dummies(win_pred, drop_first=True)\n",
    "win_pred_dummy = win_pred_dummy.astype(int); win_pred_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9967055",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf051278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into input and output\n",
    "\n",
    "# In \"x\", we're storing only predictor variables; in \"y\" we're storing target variable\n",
    "\n",
    "x= win_pred_dummy.drop(\"Deal Status Code_Won\", axis=1)\n",
    "\n",
    "y = win_pred_dummy.loc[:,\"Deal Status Code_Won\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40258982",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import train and test split module from sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcec5bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our support vector machine model (SVC) with hyperparameters\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "sig_svc = SVC(kernel = \"sigmoid\", C = 1.0, random_state= 42, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6cbd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training our model\n",
    "\n",
    "sig_svc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddbc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vector of our trained model\n",
    "\n",
    "sig_svc.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the support vectors\n",
    "\n",
    "len(sig_svc.support_vectors_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdde9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting our trained model to our test data \n",
    "\n",
    "sig_svc_pred_test = sig_svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1200248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "\n",
    "sig_svc_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbca000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the total of True and False outputs in our test model\n",
    "\n",
    "np.unique(sig_svc_pred_test, return_counts= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2563ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of True and False in our original \"y\" dataset\n",
    "\n",
    "y_test[0:].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af56a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "conf_mat = metrics.confusion_matrix(y_test, sig_svc_pred_test)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [\"0\",\"1\"])\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "# It is predicting more losses than wins (as expected since our dataset was slightly skewed towards \"loss\" (refer line 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f365e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, sig_svc_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e2dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "#Prediction on test data based on the number of thresholds and False Positive Rate and True Positive Rate\n",
    "\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(y_test, sig_svc_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "plt.figure()\n",
    "\n",
    "#Plot false positive and true positive rate and area under the curve value\n",
    "\n",
    "plt.plot(false_positive_rate, true_positive_rate, label = \"Support Vector (area = %0.2f)\" % roc_auc)\n",
    "\n",
    "#single dotted red line\n",
    "\n",
    "plt.plot([0,1], [0,1], \"r--\")\n",
    "\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7764636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of our test model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, sig_svc_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c550281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radial basis kernel function\n",
    "\n",
    "rbf_svc = SVC(kernel = \"rbf\", gamma = 0.7, C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627258f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model for training data\n",
    "\n",
    "rbf_svc.fit(x_train,y_train)\n",
    "rbf_svc.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f61ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "\n",
    "rbf_svc_pred_test = rbf_svc.predict(x_test)\n",
    "rbf_svc_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d810ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the counts of predicted output\n",
    "\n",
    "np.unique(rbf_svc_pred_test, return_counts= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fb1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0:].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec0d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confustion matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "conf_mat_rbf = metrics.confusion_matrix(y_test, rbf_svc_pred_test)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mat_rbf, display_labels = [\"0\",\"1\"])\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ea7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, rbf_svc_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c2167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "\n",
    "#Prediction on test data based on the number of thresholds and False Positive Rate and True Positive Rate\n",
    "\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(y_test, rbf_svc_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "plt.figure()\n",
    "\n",
    "#Plot false positive and true positive rate and area under the curve value\n",
    "\n",
    "plt.plot(false_positive_rate, true_positive_rate, label = \"Support Vector (area = %0.2f)\" % roc_auc)\n",
    "\n",
    "#single dotted red line\n",
    "\n",
    "plt.plot([0,1], [0,1], \"r--\")\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of our test model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, rbf_svc_pred_test)\n",
    "\n",
    "# The accuracy of rbf model is significantly better than sigmoid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the predictor test dataset and creating a new column \"Prediction\" by fitting the model to the dataset\n",
    "\n",
    "df = x_test.copy()\n",
    "df[\"Prediction\"] = rbf_svc_pred_test; df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataset by joining our original dataset with the prediction column\n",
    "\n",
    "win = win_pred.join(df.Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b21a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3530f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "win[\"Prediction\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ded984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wins\n",
    "\n",
    "win.loc[win[\"Prediction\"]==1, \"Deal Cost\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19440daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses\n",
    "\n",
    "win.loc[win[\"Prediction\"]==0, \"Deal Cost\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting only those deals which are won\n",
    "\n",
    "deals_won = win.loc[win[\"Prediction\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440adf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "deals_won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4654a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping to see the combo which has done the most successful deals\n",
    "\n",
    "deals_won.groupby(\"Prediction\")[[\"VP Name\",\"Manager Name\"]].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e85c85",
   "metadata": {},
   "source": [
    "# Oversampling (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1babe629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can clearly see that around 64% of the predictions are \"Lost\", and hence the model can be slightly biased towards \"Lost\" prediction\n",
    "\n",
    "win_pred[\"Deal Status Code\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c4254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables again\n",
    "\n",
    "win_pred_dummy_2 = pd.get_dummies(win_pred, drop_first=True); win_pred_dummy_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676540e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "\n",
    "win_pred_loss_0 = win_pred_dummy_2[win_pred_dummy_2[\"Deal Status Code_Won\"]==0]\n",
    "win_pred_wins_1 = win_pred_dummy_2[win_pred_dummy_2[\"Deal Status Code_Won\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af6325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling the minority class by generating synthetic data \n",
    "\n",
    "win_pred_wins_1_oversampling = win_pred_wins_1.sample(6306, replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388767ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating\n",
    "\n",
    "win_pred_over_sampled = pd.concat([win_pred_loss_0, win_pred_wins_1_oversampling],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ef7b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the target variable count for both possibilities are equal\n",
    "\n",
    "win_pred_over_sampled[\"Deal Status Code_Won\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b940c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the oversampled data set into input and output\n",
    "\n",
    "x1 = win_pred_over_sampled.drop(\"Deal Status Code_Won\", axis = 1)\n",
    "y1 = win_pred_over_sampled.loc[:, \"Deal Status Code_Won\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8a0511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import train and test split from sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(x1,y1,test_size = 0.3, random_state = 42, stratify=y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b582d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our model\n",
    "\n",
    "rbf_svc_2 = SVC(kernel = \"rbf\", C=1.0, gamma = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c65d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with training data\n",
    "\n",
    "rbf_svc_2.fit(x1_train, y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model for testing data\n",
    "\n",
    "smote_pred_test = rbf_svc_2.predict(x1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26211acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "conf_mat = metrics.confusion_matrix(y1_test, smote_pred_test)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [\"0\",\"1\"])\n",
    "cm_display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y1_test, smote_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test data based on the number of thresholds and False Positive Rate and True Positive Rate\n",
    "\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(y1_test, smote_pred_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "plt.figure()\n",
    "\n",
    "# Plot false positive and true positive rate and area under the curve value\n",
    "\n",
    "plt.plot(false_positive_rate, true_positive_rate, label = \"Support Vector (area = %0.2f)\" % roc_auc)\n",
    "\n",
    "# Single dotted red line\n",
    "\n",
    "plt.plot([0,1], [0,1], \"r--\")\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e1d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of our test model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y1_test, smote_pred_test)\n",
    "\n",
    "# The accuracy has improved from previous model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d8b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the predictor test dataset and creating a new column \"Prediction\" by fitting the model to the dataset\n",
    "\n",
    "df_1 = x1_test.copy()\n",
    "df_1[\"Prediction\"] = rbf_svc_2.predict(x1_test)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataset by joining the prediction column with our original dataset\n",
    "\n",
    "win_1 = win_pred.join(df_1.Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582ea8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_1[\"Prediction\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f1e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winnings total\n",
    "\n",
    "win_1.loc[win_1[\"Prediction\"]==1, \"Deal Cost\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bf0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total loss\n",
    "\n",
    "win_1.loc[win_1[\"Prediction\"]==0, \"Deal Cost\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting only those deals which are won\n",
    "\n",
    "deals_won_1 = win_1.loc[win_1[\"Prediction\"]==1]; deals_won_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16502ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping to see the combo which has done the most successful deals\n",
    "\n",
    "deals_won_1.groupby(\"Prediction\")[[\"VP Name\",\"Manager Name\"]].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96517690",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0647eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our original dataset with dummy variables coding\n",
    "\n",
    "win_pred_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a35bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into input and output\n",
    "\n",
    "# In \"x\", we're storing only predictor variables; in \"y\" we're storing target variable\n",
    "\n",
    "x2= win_pred_dummy.drop(\"Deal Status Code_Won\", axis=1)\n",
    "\n",
    "y2 = win_pred_dummy.loc[:,\"Deal Status Code_Won\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e934b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the datasets into training and testing input and ouput variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x2_train,x2_test,y2_train,y2_test=train_test_split(x2,y2,test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc94c42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_train.shape,x2_test.shape,y2_train.shape,y2_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855122e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg=LogisticRegression()\n",
    "log_reg.fit(x2_train,y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be068f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eded7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929303b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=log_reg.predict(x2_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedca67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict_proba(x2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba90c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y2_test,log_reg.predict(x2_test))\n",
    "fpr, tpr, thresholds = roc_curve(y2_test,log_reg.predict_proba(x2_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y2_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd94a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "conf_mat = metrics.confusion_matrix(y2_test, prediction)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [\"0\",\"1\"])\n",
    "cm_display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe21015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y2_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd925d9",
   "metadata": {},
   "source": [
    "# Oversampling (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d3ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the oversampled dataset that we've already done for SVM\n",
    "\n",
    "win_pred_over_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b446294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the oversampled data set into input and output\n",
    "\n",
    "x3 = win_pred_over_sampled.drop(\"Deal Status Code_Won\", axis = 1)\n",
    "y3 = win_pred_over_sampled.loc[:, \"Deal Status Code_Won\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee806a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the datasets into training and testing input and ouput variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x3_train,x3_test,y3_train,y3_test=train_test_split(x3,y3,test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x3_train.shape,x3_test.shape,y3_train.shape,y3_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89864045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg=LogisticRegression()\n",
    "log_reg.fit(x3_train,y3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a21525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c67157",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_2=log_reg.predict(x3_test)\n",
    "prediction_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict_proba(x3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c51d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y3_test,log_reg.predict(x3_test))\n",
    "fpr, tpr, thresholds = roc_curve(y3_test,log_reg.predict_proba(x3_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09058f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489532a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y3_test,prediction_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319c5b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "conf_mat = metrics.confusion_matrix(y3_test, prediction_2)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [\"0\",\"1\"])\n",
    "cm_display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of our predictions\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y3_test,prediction_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b2df6",
   "metadata": {},
   "source": [
    "# XGBoost Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530480dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our original dataset with dummy variables coding\n",
    "\n",
    "win_pred_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad85d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into input and output\n",
    "\n",
    "# In \"x\", we're storing only predictor variables; in \"y\" we're storing target variable\n",
    "\n",
    "x4= win_pred_dummy.drop(\"Deal Status Code_Won\", axis=1)\n",
    "\n",
    "y4 = win_pred_dummy.loc[:,\"Deal Status Code_Won\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e10ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the datasets into training and testing input and ouput variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x4_train,x4_test,y4_train,y4_test=train_test_split(x4,y4,test_size=0.3, random_state=42, stratify=y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdcf96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc34c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model with parameters\n",
    "\n",
    "clf=XGBClassifier(max_depth=7, learning_rate=0.1, n_estimators=1000, objective='binary:logistic', booster='gbtree', subsample=0.8, min_child_weight=1, colsample_bytree=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53514f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da0ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model on training data\n",
    "\n",
    "XGB=clf.fit(x4_train,y4_train)\n",
    "prediction_4=XGB.predict(x4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring accuracy on Testing Data\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y4_test, prediction_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f49dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "conf_mat = metrics.confusion_matrix(y4_test, prediction_4)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [\"0\",\"1\"])\n",
    "cm_display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4943c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of our predictions\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y4_test, prediction_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56089353",
   "metadata": {},
   "source": [
    "# XGBoost Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05728700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampled dataset\n",
    "\n",
    "win_pred_over_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0704188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the oversampled data set into input and output\n",
    "\n",
    "x5 = win_pred_over_sampled.drop(\"Deal Status Code_Won\", axis = 1)\n",
    "y5 = win_pred_over_sampled.loc[:, \"Deal Status Code_Won\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c2065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the datasets into training and testing input and ouput variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x5_train,x5_test,y5_train,y5_test=train_test_split(x5,y5,test_size=0.3, random_state=42, stratify=y5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086fa9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model with parameters\n",
    "\n",
    "clf=XGBClassifier(max_depth=7, learning_rate=0.1, n_estimators=1000, objective='binary:logistic', booster='gbtree', subsample=0.8, min_child_weight=1, colsample_bytree=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model on training data\n",
    "\n",
    "XGB=clf.fit(x5_train,y5_train)\n",
    "prediction_5=XGB.predict(x5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ecb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measuring accuracy on Testing Data\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y5_test, prediction_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080eedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "conf_mat = metrics.confusion_matrix(y5_test, prediction_5)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [\"0\",\"1\"])\n",
    "cm_display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ce224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of our prediction \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y5_test, prediction_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81b9540",
   "metadata": {},
   "source": [
    "# XGBoost K-Cross Validation (original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f2f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our original dataset with dummy variables coding\n",
    "\n",
    "win_pred_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db24321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into input and output\n",
    "\n",
    "# In \"x\", we're storing only predictor variables; in \"y\" we're storing target variable\n",
    "\n",
    "x6= win_pred_dummy.drop(\"Deal Status Code_Won\", axis=1)\n",
    "\n",
    "y6 = win_pred_dummy.loc[:,\"Deal Status Code_Won\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3493fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f17cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data matrix \"DMatrix\"\n",
    "\n",
    "data_dmatrix = xgb.DMatrix(data=x6, label=y6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the datasets into training and testing input and ouput variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x6_train,x6_test,y6_train,y6_test=train_test_split(x6,y6,test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30437c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "params={\"objective\":'binary:logistic',\"alpha\": 10,\"learning_rate\": 1.0,\"n_estimators\":1000, \"max_depth\":7}         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14842381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier\n",
    "\n",
    "clf_1 = XGBClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the training model\n",
    "\n",
    "clf_1.fit(x6_train, y6_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a25b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_1.predict(x6_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1111944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring accuracy on Testing Data\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y6_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "conf_mat = metrics.confusion_matrix(y6_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [\"0\",\"1\"])\n",
    "cm_display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a0e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of our prediction \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y6_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import cv\n",
    "\n",
    "xgb_cv = cv(dtrain=data_dmatrix, params = params, nfold=6, num_boost_round=1000, early_stopping_rounds=500, metrics=\"auc\", as_pandas=True, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489bf9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1034037d",
   "metadata": {},
   "source": [
    "# XGBoost K-Cross Validation (Oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f9bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the oversampled data set into input and output\n",
    "\n",
    "x7 = win_pred_over_sampled.drop(\"Deal Status Code_Won\", axis = 1)\n",
    "y7 = win_pred_over_sampled.loc[:, \"Deal Status Code_Won\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13879eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the datasets into training and testing input and ouput variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x7_train,x7_test,y7_train,y7_test=train_test_split(x7,y7,test_size=0.3, random_state=42, stratify=y7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data matrix \"DMatrix\"\n",
    "\n",
    "data_dmatrix_2 = xgb.DMatrix(data=x7, label=y7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c5cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "params_2={\"objective\":'binary:logistic',\"alpha\": 10,\"learning_rate\": 1.0,\"n_estimators\":1000, \"max_depth\":7}         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5fdac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier\n",
    "\n",
    "clf_2 = XGBClassifier(**params_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d43ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the training model\n",
    "\n",
    "clf_2.fit(x7_train, y7_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae8d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = clf_2.predict(x7_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring accuracy on Testing Data\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y7_test, y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736389cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "conf_mat = metrics.confusion_matrix(y7_test, y_pred_2)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [\"0\",\"1\"])\n",
    "cm_display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee49de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of our prediction \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y7_test, y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import cv\n",
    "\n",
    "xgb_cv_2 = cv(dtrain=data_dmatrix_2, params = params, nfold=7, num_boost_round=1000, early_stopping_rounds=500, metrics=\"auc\", as_pandas=True, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555308dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae9994b",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our original dataset with dummy variables coding\n",
    "\n",
    "win_pred_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b713d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into input and output\n",
    "\n",
    "# In \"x\", we're storing only predictor variables; in \"y\" we're storing target variable\n",
    "\n",
    "x8= win_pred_dummy.drop(\"Deal Status Code_Won\", axis=1)\n",
    "\n",
    "y8 = win_pred_dummy.loc[:,\"Deal Status Code_Won\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b1791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the datasets into training and testing input and ouput variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x8_train,x8_test,y8_train,y8_test=train_test_split(x8,y8,test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018e7766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing random forest classifier to build our model\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb5c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_3 = RandomForestClassifier(n_estimators = 100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b89927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to our training data\n",
    "\n",
    "clf_3.fit(x8_train, y8_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction for our test data\n",
    "\n",
    "pred_3 = clf_3.predict(x8_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a348c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring accuracy on Testing Data\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y8_test, pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1965df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "conf_mat = metrics.confusion_matrix(y8_test, pred_3)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [\"0\",\"1\"])\n",
    "cm_display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of our prediction \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y8_test, pred_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d94a1b",
   "metadata": {},
   "source": [
    "# Random Forest (Oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the oversampled data set into input and output\n",
    "\n",
    "x9 = win_pred_over_sampled.drop(\"Deal Status Code_Won\", axis = 1)\n",
    "y9 = win_pred_over_sampled.loc[:, \"Deal Status Code_Won\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcff759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the datasets into training and testing input and ouput variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x9_train,x9_test,y9_train,y9_test=train_test_split(x9,y9,test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80daecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_4 = RandomForestClassifier(n_estimators = 100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff02051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to our training data\n",
    "\n",
    "clf_4.fit(x9_train, y9_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction for our test data\n",
    "\n",
    "pred_4 = clf_4.predict(x9_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47567c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring accuracy on Testing Data\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y9_test, pred_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f75dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "conf_mat = metrics.confusion_matrix(y9_test, pred_4)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mat, display_labels = [\"0\",\"1\"])\n",
    "cm_display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of our prediction \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y9_test, pred_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99331f01",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f5d90",
   "metadata": {},
   "source": [
    "# 1. The accuracy of SVC radial kernel model oversampling is the highest ~90% followed by random forest oversampling ~89%\n",
    "\n",
    "# 2. Logistic Regression has performed very poorly\n",
    "\n",
    "# 3. XGBoost has performed decently, but hyperparameter tuning will have to be done on all ML algorithms to check if the accuracy can be improved\n",
    "\n",
    "# 4. Another consideration/iteration of the program that can be done is to see if the dropped features can have any impact/improve the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b783c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
